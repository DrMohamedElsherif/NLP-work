{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<span style=\"color:red\">Abgegeben von (Name, Vorname):</span> \n",
    "**Elsherif, Mohamed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T15:30:53.213570Z",
     "start_time": "2021-12-14T15:30:53.211153Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Immer griffbereit:\n",
    "- Website: https://www.nltk.org/\n",
    "- Buch: https://www.nltk.org/book/\n",
    "- Module: https://www.nltk.org/py-modindex.html\n",
    "- Beispiele: http://www.nltk.org/howto/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<font size=\"6\"><strong>7. Sitzung: POS-Tagging und N-Gram-Modelle</strong></font>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das POS-Tagging ist wie die Tokenisierung ein wichtiger Vorverarbeitungsschritt bei der Analyse von Sätzen. In diesem Notebook werden wir einfache POS-Tagger mittels N-Gram-Modelle implementieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Was ist POS-Tagging?\n",
    "\n",
    "Unter POS-Tagging versteht man die **Wortartenklassifizierung** von Wordtoken anhand sogenannter **POS-Tags**. Zum Beispiel wird im ersten Satz des Brown Corpus ein Wordtoken *place* mit dem POS-Tag `NN` versehen, um anzuzeigen, dass es sich hierbei um ein Nomen handelt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T15:31:02.408617Z",
     "start_time": "2021-12-14T15:31:02.258830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('place', 'NN')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "brown.tagged_sents()[0][23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Das POS-Tagging hilft, von der konkreten Wortform eines Tokens zu abstrahieren und (statistische) Zusammenhänge zwischen Worttoken leichter zu erkennen. Damit lässt sich z.B. leichter der grobe Bauplan eines Satzes angeben, ohne alle möglichen Wortformen aufzählen zu müssen. Wir können dann Beschreibungen benutzen wie: \n",
    "\n",
    "- Ein Nomen steht oft direkt hinter einem Artikel.\n",
    "- Ein Artikel steht nie direkt hinter einem Artikel (zumindest im Englischen).\n",
    "- Ein Adjektiv steht oft zwischen einem Artikel und einem Nomen. \n",
    "- usw.\n",
    "\n",
    "POS-Tags könnnen auch bei Suchanfragen verwendet werden, etwa um [Konkordanzen](#Appendix:-Konkordanzen) zu erstellen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Wortarten und Tagsets\n",
    "\n",
    "Ein POS-Tag zeigt die [**Wortart**](https://de.wikipedia.org/wiki/Wortart) (engl.: part of speech) eines Wortokens an. Die Wortart wird abhängig von der Morphologie (d.h. Wortform), der Syntax (d.h. der Umgebung des Worttokens im Satz) und der Semantik (d.h. Wortbedeutung) bestimmt und soll wichtige Eigenschaften dieser Aspekte zusammenfassen.\n",
    "\n",
    "**Beispiel:** Nomen (Substantiv, engl.: noun)\n",
    "- *Morphologie*: Ein Nomen hat bestimmte Merkmale (z.B. Kasus, Numerus, Genus), ggf. erkennbar an bestimmten Derivationsaffixen (*open*-***ness***) und Flexionsaffixen (*house*-***s***).\n",
    "- *Syntax*: Ein Nomen steht hinter einem Artikel oder Adjektiv und bildet mit diesen eine [Konstituente](https://de.wikipedia.org/wiki/Konstituente) oder [Phrase](https://de.wikipedia.org/wiki/Phrase_(Linguistik)).\n",
    "- *Semantik*: Ein Nomen bezeichnet (im prototypischen Fall) eine Sache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Je nach Auswahl und Granularität der morphologischen, syntaktischen und semantischen Eigenschaften können unterschiedliche **Tagsets** definiert werden. Für das Englische gibt es zum Beispiel drei sehr weit verbreitete Tagsets:\n",
    "\n",
    "- [Tagset des Brown Corpus](https://varieng.helsinki.fi/CoRD/corpora/BROWN/tags.html) (87 Tags)\n",
    "- [Tagset der Penn Treebank (PTB)](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) (36 POS-Tags + 13 sonstige Tags)\n",
    "- [Tagset des British National Corpus (BNC)](http://www.natcorp.ox.ac.uk/docs/c5spec.html) (62 Tags)\n",
    "\n",
    "Für das Deutsche hat sich das [Stuttgart-Tübingen Tagset (STTS)](https://homepage.ruhr-uni-bochum.de/Stephen.Berman/Korpuslinguistik/Tagsets-STTS.html) durchgesetzt.\n",
    "\n",
    "Der Umfang eines Tagsets hängt nicht zuletzt davon ab, wie reich die Morphologie einer Sprache ist. Die Tagsets für morphologisch reichere Sprachen sind oft (aber nicht immer) umfangreicher als Tagsets für morphologisch einfachere Sprachen wie das Englische. Z.B. gibt es ein [Tagset für das Tschechische](https://www.sketchengine.eu/tagset-reference-for-czech/), das 4288 POS-Tags enthält.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "NLTK stellt bei den getaggten Corpora neben dem ursprünglichen Tagset auch ein sogenanntes **Universelles Tagset** zur Verfügung. Dieses enthält im Wesentlichen die Hauptwortarten, die auch im Schulunterricht behandelt werden (und aus der griechischen/lateinischen Grammatikschreibung stammen). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "NLTK dokumentiert die genutzen POS-Tags aus dem Universellen Tagset folgendermaßen:\n",
    "\n",
    "| **Tag** | **Meaning**         | **English Examples**                     |\n",
    "| :------- | :------------------- | :---------------------------------------- |\n",
    "| `ADJ`   | adjective           | *new, good, high, special, big, local*   |\n",
    "| `ADP`   | adposition          | *on, of, at, with, by, into, under*      |\n",
    "| `ADV`   | adverb              | *really, already, still, early, now*     |\n",
    "| `CONJ`  | conjunction         | *and, or, but, if, while, although*      |\n",
    "| `DET`   | determiner, article | *the, a, some, most, every, no, which*   |\n",
    "| `NOUN`  | noun                | *year, home, costs, time, Africa*        |\n",
    "| `NUM`   | numeral             | *twenty-four, fourth, 1991, 14:24*       |\n",
    "| `PRT`   | particle            | *at, on, out, over, per, that, up, with*  |\n",
    "| `PRON`  | pronoun             | *he, their, her, its, my, I, us*         |\n",
    "| `VERB`  | verb                | *is, say, told, given, playing, would*   |\n",
    "| `.`     | punctuation marks   | *. , ; \\!*                               |\n",
    "| `X`     | other               | *ersatz, esprit, dunno, gr8, univeristy* |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Das vollständige Universelle Tagset enthält außerdem `AUX` (\"auxiliary verb\"), `INTJ` (\"interjection\"), `PROPN` (\"proper noun\"), `SCONJ` (\"subordinating conjunction\") und `SYM` (\"symbol\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Man beachte: Mehrworteinheiten wie *Windows 7* werden nicht als ganzes getagt, sondern nur jeweils die darin enthaltenen Wörter. Das Ergebnis wäre hier `PROPN` und `NUM`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Das Universelle Tagset kann mittels der Option `tagset='universal'` aufgerufen werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T15:42:46.165472Z",
     "start_time": "2021-12-14T15:42:46.073182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown Tagset:\n",
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')]\n",
      "\n",
      "Universal Tagset:\n",
      "[('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/dr.elsherif/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('universal_tagset')\n",
    "print(\"Brown Tagset:\\n{}\\n\".format(brown.tagged_sents()[0]))\n",
    "print(\"Universal Tagset:\\n{}\".format(brown.tagged_sents(tagset='universal')[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Das Universelle Tagset wurde im Rahmen der [**Universal-Dependencies-Initiative**](https://universaldependencies.org/) definiert und soll die Tagsets sprachübergreifend vereinheitlichen (weitere Informationen gibt es [hier](https://universaldependencies.org/u/pos/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <span style=\"color:red\">Aufgaben I: Ambiguität beim POS-Tagging</span>\n",
    "\n",
    "Für das POS-Tagging reicht es oft nicht aus, nur die Wortform eines Tokens zu betrachten, denn oft kann dieselbe Wortformen unterschiedlichen POS-Tags zugeordnet werden. Wortformen sind also hinsichtlich der POS-Tags mehrdeutig/ambig.\n",
    "\n",
    "<span style=\"color:red\">A1:</span> Bestimmen Sie den Grad der POS-Tag-Ambiguität der Wortformen im Brown Corpus bezogen auf das Universelle Tagset anhand der folgenden Kennzahlen:\n",
    "- durchschnittliche Anzahl der POS-Tags einer Wortform\n",
    "- die 10 Wortformen mit der höchsten Anzahl unterschiedlicher POS-Tags\n",
    "- der %-Anteil der ambigen Wortformen bei den Wortformen\n",
    "- durchschnittliche Anzahl der möglichen POS-Tags eines Worttokens **aufgrund seiner Wortform**\n",
    "- der %-Anteil der Worttoken mit ambiger Wortform\n",
    "\n",
    "Hinweis: Hier können Sie z.B. [FreqDist](https://www.nltk.org/api/nltk.probability.html?highlight=freqdist#nltk.probability.FreqDist) verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T15:47:57.588186Z",
     "start_time": "2021-12-14T15:47:52.514066Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lösung A1\n",
    "from nltk.probability import FreqDist\n",
    "from collections import defaultdict\n",
    "# Brown corpus mit universal POS tags \n",
    "tagged_words = brown.tagged_words(tagset=\"universal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T15:54:12.205117Z",
     "start_time": "2021-12-14T15:54:12.131512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Average number of POS tags per word form: 1.0749372678911975\n"
     ]
    }
   ],
   "source": [
    "# Durchschnittliche Anzahl der POS-Tags einer Wortform\n",
    "word_to_tags = defaultdict(set)\n",
    "for word, tag in tagged_words:\n",
    "    word_to_tags[word.lower()].add(tag)\n",
    "\n",
    "avg_tags_per_word_form = sum(len(tags) for tags in word_to_tags.values()) / len(word_to_tags)\n",
    "\n",
    "print(f\"1. Average number of POS tags per word form: {avg_tags_per_word_form:}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T15:55:14.716127Z",
     "start_time": "2021-12-14T15:55:14.658292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Top 10 ambiguous word forms (word -> tags):\n",
      "   down: {'PRT', 'ADV', 'ADP', 'ADJ', 'NOUN', 'VERB'}\n",
      "   that: {'ADV', 'ADP', 'DET', 'PRON', 'X'}\n",
      "   to: {'PRT', 'ADV', 'ADP', 'NOUN', 'X'}\n",
      "   well: {'ADV', 'PRT', 'ADJ', 'NOUN', 'VERB'}\n",
      "   round: {'ADV', 'ADP', 'ADJ', 'NOUN', 'VERB'}\n",
      "   damn: {'PRT', 'ADV', 'ADJ', 'NOUN', 'VERB'}\n",
      "   in: {'NOUN', 'X', 'PRT', 'ADP'}\n",
      "   many: {'ADJ', 'PRT', 'ADV', 'X'}\n",
      "   best: {'NOUN', 'ADJ', 'ADV', 'VERB'}\n",
      "   :: {'X', 'NOUN', 'ADP', '.'}\n"
     ]
    }
   ],
   "source": [
    "# Die 10 Wortformen mit der höchsten Anzahl an POS-Tags\n",
    "word_forms_sorted = sorted(word_to_tags.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "top_10_ambiguous_words = word_forms_sorted[:10]\n",
    "\n",
    "print(\"2. Top 10 ambiguous word forms (word -> tags):\")\n",
    "for word, tags in top_10_ambiguous_words:\n",
    "    print(f\"   {word}: {tags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T15:58:09.881275Z",
     "start_time": "2021-12-14T15:58:09.856332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Percentage of ambiguous word forms: 6.84131285757302%\n"
     ]
    }
   ],
   "source": [
    "# %-Anteil der ambigen Wortformen bei den Wortformen\n",
    "num_ambiguous_forms = sum(1 for tags in word_to_tags.values() if len(tags) > 1)\n",
    "percent_ambiguous_forms = (num_ambiguous_forms / len(word_to_tags)) * 100\n",
    "\n",
    "print(f\"3. Percentage of ambiguous word forms: {percent_ambiguous_forms:}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T16:00:58.485256Z",
     "start_time": "2021-12-14T16:00:54.622629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Average number of possible POS tags per word token: 1.8337096707521237\n"
     ]
    }
   ],
   "source": [
    "# Durchschnittliche Anzahl der möglichen POS-Tags eines Worttokens\n",
    "token_to_possible_tags = [len(word_to_tags[word.lower()]) for word, _ in tagged_words]\n",
    "avg_tags_per_word_token = sum(token_to_possible_tags) / len(tagged_words)\n",
    "\n",
    "print(f\"4. Average number of possible POS tags per word token: {avg_tags_per_word_token:}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T16:02:36.368946Z",
     "start_time": "2021-12-14T16:02:30.854964Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. Percentage of word tokens with ambiguous word forms: 55.67838910361077%\n"
     ]
    }
   ],
   "source": [
    "# %-Anteil der Worttoken mit ambiger Wortform\n",
    "num_ambiguous_tokens = sum(1 for tags in token_to_possible_tags if tags > 1)\n",
    "percent_ambiguous_tokens = (num_ambiguous_tokens / len(tagged_words)) * 100\n",
    "\n",
    "print(f\"5. Percentage of word tokens with ambiguous word forms: {percent_ambiguous_tokens:}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Methoden des POS-Taggings\n",
    "\n",
    "Im Folgenden werden wir einfache Methoden des POS-Taggings kennenlernen. Damit die jeweilige Leistungsfähigkeit gemessen und verglichen werden kann, müssen wir uns aber erst einmal mit dem Thema Evaluierung auseinander setzen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluierung mit Trainings- und Testdaten\n",
    "\n",
    "Für die Evaluierung der Tagger werden üblicherweise *Trainingsdaten* und *Testdaten* verwendet: \n",
    "\n",
    "- Die **Trainingsdaten** enthalten Beispiele für die konkrete Verwendung der POS-Tags. Bei überwachten Verfahren ist das die Grundlage, auf der der Tagger Zuweisung von POS-Tags lernen soll; \n",
    "- die **Testdaten** stammen meist aus demselben (hand-)annotierten Corpus, aber hier muss der Tagger die fehlenden POS-Tags einsetzen und das Ergebnis wird mit den \"Goldtags\" verglichen. \n",
    "- **Wichtig ist, dass sich Trainings- und Testdaten nicht überschneiden!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Mit Hilfe des Brown Corpus lassen sich beide Datentypen leicht erstellen, indem zum Beispiel der ersten 400 Sätze zum Testen und die restlichen ca. 4000 Sätze zum Training verwendet werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T16:05:37.306757Z",
     "start_time": "2021-12-14T16:05:36.938941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testdaten:\n",
      "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']]\n",
      "\n",
      "Trainingsdaten:\n",
      "[[('He', 'PRON'), ('is', 'VERB'), ('not', 'ADV'), ('interested', 'VERB'), ('in', 'ADP'), ('being', 'VERB'), ('named', 'VERB'), ('a', 'DET'), ('full-time', 'ADJ'), ('director', 'NOUN'), ('.', '.')], [('Noting', 'VERB'), ('that', 'ADP'), ('President', 'NOUN'), ('Kennedy', 'NOUN'), ('has', 'VERB'), ('handed', 'VERB'), ('the', 'DET'), ('Defense', 'NOUN'), ('Department', 'NOUN'), ('the', 'DET'), ('major', 'ADJ'), ('responsibility', 'NOUN'), ('for', 'ADP'), ('the', 'DET'), (\"nation's\", 'NOUN'), ('civil', 'ADJ'), ('defense', 'NOUN'), ('program', 'NOUN'), (',', '.'), ('Mr.', 'NOUN'), ('Hawksley', 'NOUN'), ('said', 'VERB'), ('the', 'DET'), ('federal', 'ADJ'), ('government', 'NOUN'), ('would', 'VERB'), ('pay', 'VERB'), ('half', 'PRT'), ('the', 'DET'), ('salary', 'NOUN'), ('of', 'ADP'), ('a', 'DET'), ('full-time', 'ADJ'), ('local', 'ADJ'), ('director', 'NOUN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import untag\n",
    "\n",
    "brown_testgold = brown.tagged_sents(categories='news', tagset='universal')[:400]\n",
    "brown_train = brown.tagged_sents(categories='news', tagset='universal')[400:]\n",
    "brown_test = [untag(sent) for sent in brown_testgold]\n",
    "\n",
    "print(\"Testdaten:\\n{}\\n\".format(brown_test[:2]))\n",
    "print(\"Trainingsdaten:\\n{}\".format(brown_train[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**ACHTUNG:** Die POS-Tagger werden immer auf einzelne tokenisierte Sätze angewandt. Die Satz- und Worttokenisierung muss also schon durchgeführt sein."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Man kann nun zwei Dinge anhand der Testdaten und der Goldtestdaten messen:\n",
    "- Wie gut werden alle Worte klassifiziert?  $\\Rightarrow$ **Accuracy** (global precision)\n",
    "- Wie gut funktioniert die Klassifikation bei einzelnen POS-Tags? $\\Rightarrow$ **Precision**, **Recall**, **F1 Measure**\n",
    "\n",
    "\\begin{align} \n",
    "R =&~ \\frac{\\#\\text{correctly assigned POS tags}}{\\#\\text{POS tags in gold data}} \\\\\n",
    "P =&~ \\frac{\\#\\text{correctly assigned POS tags}}{\\#\\text{POS tags assigned}} \\\\\n",
    "F1 =&~ \\frac{2RP}{R+P}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Man sieht auch manchmal, dass Precision, Recall und F1-Measure auf das ganze Testset angewandt werden. Das ist beim POS-Tagging aber äquivalent zur Accuracy, da jedes Wort genau ein POS-Tag erhalten muss. Es gilt also in diesem Fall:\n",
    "\n",
    "\\begin{align}\n",
    "\\#\\text{POS tags assigned} =&~ \\#\\text{POS tags in gold data} \n",
    "\\end{align}\n",
    "\n",
    "Und damit gilt auch $R = P$, so dass wir F1 folgendermaßen auflösen können:\n",
    "\n",
    "\\begin{align}\n",
    "F1 = \\frac{2RP}{R+P} = \\frac{2P^2}{2P} = P\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Default-Tagger\n",
    "\n",
    "Der Default-Tagger weist immer dasselbe POS-Tag zu, egal welches Wort vorliegt. Im Unterschied zu einem Random-Tagger, der aus den POS-Tags zufällig eines auswählt, kann der Default-Tagger aus den Testdaten immerhin \"lernen\", welches POS-Tag am häufigsten ist.\n",
    "\n",
    "NLTK enthält bereits eine Klasse `DefaultTagger`, mit der in wenigen Zeilen ein Default-Tagger erstellt werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T16:10:21.889441Z",
     "start_time": "2021-12-14T16:10:21.881668Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'POS'),\n",
       " ('Fulton', 'POS'),\n",
       " ('County', 'POS'),\n",
       " ('Grand', 'POS'),\n",
       " ('Jury', 'POS'),\n",
       " ('said', 'POS'),\n",
       " ('Friday', 'POS'),\n",
       " ('an', 'POS'),\n",
       " ('investigation', 'POS'),\n",
       " ('of', 'POS'),\n",
       " (\"Atlanta's\", 'POS'),\n",
       " ('recent', 'POS'),\n",
       " ('primary', 'POS'),\n",
       " ('election', 'POS'),\n",
       " ('produced', 'POS'),\n",
       " ('``', 'POS'),\n",
       " ('no', 'POS'),\n",
       " ('evidence', 'POS'),\n",
       " (\"''\", 'POS'),\n",
       " ('that', 'POS'),\n",
       " ('any', 'POS'),\n",
       " ('irregularities', 'POS'),\n",
       " ('took', 'POS'),\n",
       " ('place', 'POS'),\n",
       " ('.', 'POS')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import DefaultTagger\n",
    "dt = DefaultTagger('POS')\n",
    "dt.tag(brown_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Nun muss nur noch das häufigste POS-Tag in den Trainingsdaten ermittelt werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T16:10:55.828963Z",
     "start_time": "2021-12-14T16:10:55.353216Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'NOUN'),\n",
       " ('Fulton', 'NOUN'),\n",
       " ('County', 'NOUN'),\n",
       " ('Grand', 'NOUN'),\n",
       " ('Jury', 'NOUN'),\n",
       " ('said', 'NOUN'),\n",
       " ('Friday', 'NOUN'),\n",
       " ('an', 'NOUN'),\n",
       " ('investigation', 'NOUN'),\n",
       " ('of', 'NOUN'),\n",
       " (\"Atlanta's\", 'NOUN'),\n",
       " ('recent', 'NOUN'),\n",
       " ('primary', 'NOUN'),\n",
       " ('election', 'NOUN'),\n",
       " ('produced', 'NOUN'),\n",
       " ('``', 'NOUN'),\n",
       " ('no', 'NOUN'),\n",
       " ('evidence', 'NOUN'),\n",
       " (\"''\", 'NOUN'),\n",
       " ('that', 'NOUN'),\n",
       " ('any', 'NOUN'),\n",
       " ('irregularities', 'NOUN'),\n",
       " ('took', 'NOUN'),\n",
       " ('place', 'NOUN'),\n",
       " ('.', 'NOUN')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Default-Tagger beim Lernen\n",
    "mfpos = FreqDist([tag for sent in brown_train for word,tag in sent]).most_common(1)[0][0]\n",
    "\n",
    "dt = DefaultTagger(mfpos)\n",
    "dt.tag(brown_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Für die Evaluation steht bei Tagger-Objekten glücklicherweise die Methode [`accuracy(gold)`](https://www.nltk.org/api/nltk.tag.api.html?highlight=evaluate#nltk.tag.api.TaggerI.accuracy) zur Verfügung. Bitte daran denken, dass jetzt die Goldtestdaten verwendet werden müssen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T16:11:20.421139Z",
     "start_time": "2021-12-14T16:11:20.352102Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30919679156136687"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.accuracy(brown_testgold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine Accuracy von 31,8% ist wirklich sehr niedrig – vor allem da wir wissen, dass nur ein POS-Tag verwendet wird. Der Informationsgewinn ist also gleich null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regexp-Tagger\n",
    "\n",
    "Während der Default-Tagger die Wortform ignoriert, geht es beim Regexp-Tagger darum, allein aus der Wortform das POS-Tag zu erschließen. Trainingsdaten werden also nicht benötigt. \n",
    "\n",
    "NLTK enthält hierfür die Klasse `RegexpTagger`, die wie `DefaultTagger` bedient werden kann (beide implementieren das Interface `TaggerI`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T16:11:48.293622Z",
     "start_time": "2021-12-14T16:11:48.275520Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DET'),\n",
       " ('Fulton', 'NOUN'),\n",
       " ('County', 'NOUN'),\n",
       " ('Grand', 'NOUN'),\n",
       " ('Jury', 'NOUN'),\n",
       " ('said', 'NOUN'),\n",
       " ('Friday', 'NOUN'),\n",
       " ('an', 'DET'),\n",
       " ('investigation', 'NOUN'),\n",
       " ('of', 'NOUN'),\n",
       " (\"Atlanta's\", 'NOUN'),\n",
       " ('recent', 'NOUN'),\n",
       " ('primary', 'NOUN'),\n",
       " ('election', 'NOUN'),\n",
       " ('produced', 'VERB'),\n",
       " ('``', '.'),\n",
       " ('no', 'NOUN'),\n",
       " ('evidence', 'NOUN'),\n",
       " (\"''\", '.'),\n",
       " ('that', 'NOUN'),\n",
       " ('any', 'NOUN'),\n",
       " ('irregularities', 'VERB'),\n",
       " ('took', 'NOUN'),\n",
       " ('place', 'NOUN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import RegexpTagger\n",
    "rt = RegexpTagger([\n",
    "    (r'(The|the|A|a|An|an)$', 'DET'),      # articles\n",
    "    (r'.*able$', 'ADJ'),                   # adjectives\n",
    "    (r'.*ness$', 'NOUN'),                  # nouns formed from adjectives\n",
    "    (r'.*ly$', 'ADV'),                     # adverbs\n",
    "    (r'.*ing$', 'VERB'),                   # gerunds\n",
    "    (r'.*ed$', 'VERB'),                    # simple past\n",
    "    (r'.*es$', 'VERB'),                    # 3rd singular present\n",
    "    (r'.*ould$', 'VERB'),                  # modals\n",
    "    (r'.*\\'s$', 'NOUN'),                   # possessive nouns\n",
    "    (r'.*s$', 'NOUN'),                     # plural nouns\n",
    "    (r'^-?[0-9]+(\\.[0-9]+)?$', 'NUM'),     # cardinal numbers\n",
    "    (r'(\\.|\\,|\\(|\\)|\\;|\\`\\`|\\'\\')$', '.'), # punctuation\n",
    "    (r'.*', 'NOUN'),                       # nouns (default)\n",
    "    ])\n",
    "\n",
    "rt.tag(brown_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T16:13:47.131102Z",
     "start_time": "2021-12-14T16:13:47.003706Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.566311394352269"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt.accuracy(brown_testgold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Die Accuracy ist nun deutlich erhöht, aber immer noch zu niedrig. Das grundsätzliche Problem hier ist, dass die regulären Ausdrücke nicht immer eindeutig einem POS-Tag zugeordnet werden können. Außerdem vergisst man leicht eine Regel, wodurch sich die Abdeckung verringert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Um eine bessere Ahnung davon zu bekommen, welche POS-Tags korrekt und welche fälschlicherweise vergeben wurden, lohnt sich ein Blick auf die **Konfusionsmatrix**. NLTK stell hierfür das [`ConfusionMatrix`-Modul](https://www.nltk.org/api/nltk.metrics.confusionmatrix.html?highlight=confusion%20matrix#module-nltk.metrics.confusionmatrix) zur Verfügung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T16:14:44.462380Z",
     "start_time": "2021-12-14T16:14:44.332137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |      N      V                                                C             P        |\n",
      "     |      O      E      A      D             A      A      P      O      N      R        |\n",
      "     |      U      R      D      E             D      D      R      N      U      O        |\n",
      "     |      N      B      P      T      .      J      V      T      J      M      N      X |\n",
      "-----+-------------------------------------------------------------------------------------+\n",
      "NOUN | <28.7%>  2.1%      .   0.0%      .   0.0%   0.1%      .      .      .      .      . |\n",
      "VERB |   9.1%  <6.5%>     .      .      .   0.0%   0.0%      .      .      .      .      . |\n",
      " ADP |  12.7%   0.1%     <.>     .      .      .      .      .      .      .      .      . |\n",
      " DET |   2.5%      .      .  <9.0%>     .      .      .      .      .      .      .      . |\n",
      "   . |   0.4%      .      .      . <10.4%>     .      .      .      .      .      .      . |\n",
      " ADJ |   6.4%   0.1%      .      .      .  <0.1%>  0.1%      .      .      .      .      . |\n",
      " ADV |   2.2%   0.0%      .      .      .      .  <0.8%>     .      .      .      .      . |\n",
      " PRT |   2.3%      .      .      .      .      .      .     <.>     .      .      .      . |\n",
      "CONJ |   2.2%      .      .      .      .      .      .      .     <.>     .      .      . |\n",
      " NUM |   0.9%      .      .      .      .      .      .      .      .  <1.2%>     .      . |\n",
      "PRON |   2.0%   0.0%      .      .      .      .      .      .      .      .     <.>     . |\n",
      "   X |   0.0%      .      .      .      .      .      .      .      .      .      .     <.>|\n",
      "-----+-------------------------------------------------------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import ConfusionMatrix\n",
    "\n",
    "testtags = [tag for sent in brown_test for word,tag in rt.tag(sent)]\n",
    "goldtags = [tag for sent in brown_testgold for word,tag in sent]\n",
    "\n",
    "cm = ConfusionMatrix(goldtags, testtags)\n",
    "\n",
    "print(cm.pretty_format(show_percents=True, values_in_chart=True, truncate=15, sort_by_count=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Man erkennt, dass noch immer viel zu häufig das POS-Tag `NOUN` vergeben wird (siehe Spalte \"NOUN\"), das hier also die Regeln noch verfeinert werden müssen ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lookup-Tagger oder Unigram-Tagger\n",
    "\n",
    "Die Idee beim Lookup-Tagger ist, für jedes Wort das POS-Tag in einem \"Wörterbuch\" nachzuschlagen. Dieses Wörterbuch kann ad hoc aus den Trainingsdaten generiert werden, indem für jede Wortform das häufigste POS-Tag gespeichert wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Da man hier nur einzelne Wortformen und Token betrachtet, spricht man auch von einem **Unigram-Tagger**. Wie schon beim Default-Tagger hält NLTK eine passende Klasse ([`UnigramTagger`](https://www.nltk.org/api/nltk.tag.sequential.html?highlight=unigramtagger#nltk.tag.sequential.UnigramTagger)) bereit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T16:19:48.424041Z",
     "start_time": "2021-12-14T16:19:47.742150Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DET'),\n",
       " ('Fulton', None),\n",
       " ('County', 'NOUN'),\n",
       " ('Grand', 'ADJ'),\n",
       " ('Jury', 'NOUN'),\n",
       " ('said', 'VERB'),\n",
       " ('Friday', 'NOUN'),\n",
       " ('an', 'DET'),\n",
       " ('investigation', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " (\"Atlanta's\", 'NOUN'),\n",
       " ('recent', 'ADJ'),\n",
       " ('primary', 'NOUN'),\n",
       " ('election', 'NOUN'),\n",
       " ('produced', 'VERB'),\n",
       " ('``', '.'),\n",
       " ('no', 'DET'),\n",
       " ('evidence', 'NOUN'),\n",
       " (\"''\", '.'),\n",
       " ('that', 'ADP'),\n",
       " ('any', 'DET'),\n",
       " ('irregularities', None),\n",
       " ('took', 'VERB'),\n",
       " ('place', 'NOUN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import UnigramTagger\n",
    "\n",
    "# Unigram-Tagger beim Lernen\n",
    "ut = UnigramTagger(brown_train)\n",
    "\n",
    "ut.tag(brown_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Wieder können wir mit `accuracy()` die Accuracy des Taggers anhand von Goldtestdaten ermitteln:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T16:20:51.445007Z",
     "start_time": "2021-12-14T16:20:51.391037Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8745192835952094"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ut.accuracy(brown_testgold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Das Ergebnis ist mit 88,1 % deutlich besser als beim Default-Tagger und beim Regexp-Tagger. \n",
    "\n",
    "Kann man sich damit zufrieden geben? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Leider nein! Da die Accuracy bezogen auf Worte berechnet wird, bedeutet das, dass durchschnittlich etwa jedes 10. Wort falsch getaggt wird. In jedem durchschnittlich großen Satz von 20 Worten gibt es also durchschnittlich zwei Fehler. Diese Fehlerrate ist für eine Methode, die immer noch relativ am Anfang der Verarbeitungspipeline steht, viel zu hoch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backoff: The power of many\n",
    "\n",
    "Wir müssen also weiterhin nach Verbesserungsmöglichkeiten Ausschau halten – und in der Ausgabe von `ut` fällt sofort eine Verbesserungsmöglichkeit auf: Manche Wortformen haben das POS-Tag `none` erhalten, weil sie in den Trainingsdaten nicht gesehen wurden. Das sind die sogenannten **unbekannten Worte** (\"unknown words\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |      N      V                                                C             P             N |\n",
      "     |      O      E      A      D             A      A      P      O      N      R             o |\n",
      "     |      U      R      D      E             D      D      R      N      U      O             n |\n",
      "     |      N      B      P      T      .      J      V      T      J      M      N      X      e |\n",
      "-----+--------------------------------------------------------------------------------------------+\n",
      "NOUN | <24.4%>  0.5%      .   0.0%      .   0.2%   0.0%      .      .   0.0%   0.0%   0.0%   5.7% |\n",
      "VERB |   0.7% <13.3%>  0.0%      .      .   0.0%   0.0%      .      .      .      .      .   1.6% |\n",
      " ADP |      .   0.0% <11.7%>     .      .      .   0.0%   1.0%      .      .      .      .   0.0% |\n",
      " DET |      .      .   0.0% <11.5%>     .      .      .      .      .      .      .      .   0.0% |\n",
      "   . |      .      .      .      . <10.8%>     .      .      .      .      .      .      .      . |\n",
      " ADJ |   0.2%   0.0%      .      .      .  <5.3%>  0.1%      .      .      .      .      .   1.2% |\n",
      " ADV |      .      .   0.1%   0.0%      .   0.3%  <2.3%>  0.0%      .      .      .      .   0.2% |\n",
      " PRT |      .      .   0.1%      .      .   0.0%   0.0%  <2.2%>     .      .      .      .      . |\n",
      "CONJ |      .      .      .      .      .      .      .      .  <2.2%>     .      .      .      . |\n",
      " NUM |      .      .      .      .      .      .      .      .      .  <1.7%>     .      .   0.3% |\n",
      "PRON |      .      .   0.1%      .      .      .      .      .      .      .  <1.9%>     .      . |\n",
      "   X |      .      .      .      .      .      .      .      .      .      .      .  <0.0%>  0.0% |\n",
      "None |      .      .      .      .      .      .      .      .      .      .      .      .     <.>|\n",
      "-----+--------------------------------------------------------------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import ConfusionMatrix\n",
    "\n",
    "testtags = [str(tag) for sent in brown_test for word,tag in ut.tag(sent)]\n",
    "goldtags = [tag for sent in brown_testgold for word,tag in sent]\n",
    "\n",
    "cm = ConfusionMatrix(goldtags, testtags)\n",
    "\n",
    "print(cm.pretty_format(show_percents=True, values_in_chart=True, truncate=15, sort_by_count=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Natürlich könnte man einfach das Lookup-Wörterbuch um weitere Wortformen erweitern, indem man das Lernkorpus vergrößert. Aber auch dieses Vorgehen hat Grenzen, wie das folgende Diagramm aus dem NLTK-Buch zeigt, denn es müssen immer mehr Daten (X-Achse) für immer weniger Performance-Verbesserung (Y-Achse) eingesetzt werden: \n",
    "\n",
    "![](https://www.nltk.org/images/tag-lookup.png)\n",
    "\n",
    "Vielleicht gibt es eine bessere Strategie?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Eine ganz simple Strategie besteht darin, den Regexp-Tagger dann zu verwenden, wenn der Unigram-Tagger `none` ausgeben würde. Diese Strategie eines \"geordneten Rückzugs\" nennt man **Backoff**. \n",
    "\n",
    "Praktischerweise erlaubt NLTK bei der Tagger-Konstruktion, einen Backoff per Parameter `backoff=` anzugeben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import UnigramTagger\n",
    "\n",
    "# Unigram-Tagger beim Lernen\n",
    "utplus = UnigramTagger(brown_train, backoff=rt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Die Accuracy kann dadurch deutlich zulegen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9429732996374025"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utplus.accuracy(brown_testgold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Was sich natürlich auch in der Konfusionsmatrix wiederspiegelt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T16:28:43.346516Z",
     "start_time": "2021-12-14T16:28:43.305141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |      N      V                                                C             P        |\n",
      "     |      O      E      A      D             A      A      P      O      N      R        |\n",
      "     |      U      R      D      E             D      D      R      N      U      O        |\n",
      "     |      N      B      P      T      .      J      V      T      J      M      N      X |\n",
      "-----+-------------------------------------------------------------------------------------+\n",
      "NOUN | <29.9%>  0.8%      .   0.0%      .   0.2%   0.0%      .      .   0.0%   0.0%   0.0% |\n",
      "VERB |   1.2% <14.3%>  0.0%      .      .   0.0%   0.0%      .      .      .      .      . |\n",
      " ADP |      .   0.0% <11.7%>     .      .      .   0.0%   1.0%      .      .      .      . |\n",
      " DET |   0.0%      .   0.0% <11.5%>     .      .      .      .      .      .      .      . |\n",
      "   . |      .      .      .      . <10.8%>     .      .      .      .      .      .      . |\n",
      " ADJ |   1.2%   0.1%      .      .      .  <5.4%>  0.1%      .      .      .      .      . |\n",
      " ADV |   0.0%      .   0.1%   0.0%      .   0.3%  <2.5%>  0.0%      .      .      .      . |\n",
      " PRT |      .      .   0.1%      .      .   0.0%   0.0%  <2.2%>     .      .      .      . |\n",
      "CONJ |      .      .      .      .      .      .      .      .  <2.2%>     .      .      . |\n",
      " NUM |   0.1%      .      .      .      .      .      .      .      .  <1.9%>     .      . |\n",
      "PRON |      .      .   0.1%      .      .      .      .      .      .      .  <1.9%>     . |\n",
      "   X |   0.0%      .      .      .      .      .      .      .      .      .      .  <0.0%>|\n",
      "-----+-------------------------------------------------------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testtags = [str(tag) for sent in brown_test for word,tag in utplus.tag(sent)]\n",
    "goldtags = [tag for sent in brown_testgold for word,tag in sent]\n",
    "\n",
    "cm = ConfusionMatrix(goldtags, testtags)\n",
    "\n",
    "print(cm.pretty_format(show_percents=True, values_in_chart=True, truncate=15, sort_by_count=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## N-Gram-Tagger: Mehr Kontext bitte!\n",
    "\n",
    "Eine grundlegende Einschränkung des Lookup-Taggers (bzw. des Unigram-Taggers) ist, dass der Kontext des Worttokens nicht berücksichtigt wird. Der Kontext ist aber oft wichtig, da vielen Wortformen mehr als ein POS-Tag zugewiesen werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T16:34:35.921833Z",
     "start_time": "2021-12-14T16:34:35.902251Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'DET'),\n",
       " ('fly', 'NOUN'),\n",
       " ('flies', 'VERB'),\n",
       " ('to', 'PRT'),\n",
       " ('flies', 'VERB'),\n",
       " ('in', 'ADP'),\n",
       " ('order', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('fly', 'NOUN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ut.tag(nltk.word_tokenize(\"A fly flies to flies in order to fly.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Der Ansatz des N-Gram-Taggers ist daher, den unmittelbaren, vorangehenden Kontext bei der Klassifizierung einzubeziehen. \"N\" steht für die festgelegte Größe des Kontexts: Ein **2-Gram-Tagger** (oder Bigram-Tagger) betrachtet z.B. das zu klassifizierende Wort und das POS-Tag des davorstehenden Worts. Der N-Gram-Tagger ist also eine Generalisierung des Unigram-Taggers. Schematisch kann das folgendermaßen dargestellt werden (Darstellung aus https://www.nltk.org/book/ch05.html), wobei sich die Bigramme in dieser Darstellung nur über $t_{n-1}$ und $w_n$ erstrecken: \n",
    "\n",
    "![](https://www.nltk.org/images/tag-context.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Für Bigram-Tagger bedeutet das, dass während der Lernphase für alle Token der Lerndaten Tripel der Form \n",
    "\n",
    "     (POS-Tag des vorangehenden Tokens, Wortform des Tokens, POS-Tag des Tokens) \n",
    "   \n",
    "erstellt werden. Beim Taggen eines Satzes $(w_1,t_1) \\ldots (w_k,t_k)$ wird dann für ein Token mit Wortform $w_i$ das POS-Tag $t_i$ gewählt, falls $(t_{i-1},w_i,t_i)$ für $t_{i-1}$ und $w_i$ am häufigsten in den Lerndaten vorkam. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "N-Grams von Listen können mit Hilfe des NLTK-Moduls [`ngrams`](https://www.nltk.org/api/nltk.util.html?highlight=ngram#nltk.util.ngrams) leicht erstellt werden: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T16:36:11.481318Z",
     "start_time": "2021-12-14T16:36:11.477109Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (2, 3), (3, 4)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "list(ngrams([1,2,3,4],2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Die Tupel einens N-Gram-Taggers sehen also konkret folgendermaßen aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T16:36:18.134637Z",
     "start_time": "2021-12-14T16:36:18.115078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('He', 'PRON'), ('is', 'VERB'), ('not', 'ADV'), ('interested', 'VERB'), ('in', 'ADP'), ('being', 'VERB'), ('named', 'VERB'), ('a', 'DET'), ('full-time', 'ADJ'), ('director', 'NOUN'), ('.', '.')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['PRON', 'is', 'VERB'],\n",
       " ['VERB', 'not', 'ADV'],\n",
       " ['ADV', 'interested', 'VERB'],\n",
       " ['VERB', 'in', 'ADP'],\n",
       " ['ADP', 'being', 'VERB'],\n",
       " ['VERB', 'named', 'VERB'],\n",
       " ['VERB', 'a', 'DET'],\n",
       " ['DET', 'full-time', 'ADJ'],\n",
       " ['ADJ', 'director', 'NOUN'],\n",
       " ['NOUN', '.', '.']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 2 \n",
    "input = brown_train[0]\n",
    "print(input)\n",
    "[[tag for word,tag in ngram[:-1]]+[ngram[-1][0]]+[ngram[-1][1]] for ngram in ngrams(input, n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Glücklicherweise gibt es in NLTK bereits eine passende Klasse [`NgramTagger`](https://www.nltk.org/api/nltk.tag.sequential.html?highlight=ngramtagger#nltk.tag.sequential.NgramTagger), mit der wir N-Gram-Tagger mit beliebigem N erzeugen können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T16:37:33.701456Z",
     "start_time": "2021-12-14T16:37:32.826437Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import NgramTagger\n",
    "\n",
    "# der Bigram-Tagger trainiert ...\n",
    "bigt = NgramTagger(2, brown_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Wenn wir damit den Satz von oben taggen, werden wir allerdings feststellen, dass ab dem zweiten *flies* alle Worte das POS-Tag `None` erhalten.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T16:37:39.776412Z",
     "start_time": "2021-12-14T16:37:39.770299Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'DET'),\n",
       " ('fly', 'NOUN'),\n",
       " ('flies', 'VERB'),\n",
       " ('to', 'PRT'),\n",
       " ('flies', None),\n",
       " ('in', None),\n",
       " ('order', None),\n",
       " ('to', None),\n",
       " ('fly', None),\n",
       " ('.', None)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigt.tag(nltk.word_tokenize(\"A fly flies to flies in order to fly.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Das liegt daran, dass in den Trainingsdaten *flies* nicht hinter `PRT` auftritt und deswegen als `None` klassifiziert wird. D.h. es gibt in den Lerndaten kein Tupel wie `['PRT','flies','VERB']`. Da aber `None` für den Bigram-Tagger ebenfalls ein unbekannter Kontext ist, setzt sich der Fehler bis zum Ende fort.\n",
    "\n",
    "Um dem entgegenzuwirken, muss man eine Rückfall-Strategie (\"backoff\") angeben. Das ist mit der Option `backoff` möglich und hier setzen wir einfach den Unigram-Tagger von oben ein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T16:38:58.352705Z",
     "start_time": "2021-12-14T16:38:57.341411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'DET'),\n",
       " ('fly', 'NOUN'),\n",
       " ('flies', 'VERB'),\n",
       " ('to', 'PRT'),\n",
       " ('flies', 'VERB'),\n",
       " ('in', 'ADP'),\n",
       " ('order', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('fly', 'VERB'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import NgramTagger\n",
    "\n",
    "bigt = NgramTagger(2, brown_train,backoff=ut)\n",
    "bigt.tag(nltk.word_tokenize(\"A fly flies to flies in order to fly.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In der Evaluation sehen wir, dass der Bigram-Tagger mit dem Unigram-Tagger als Backoff zu leicht verbesserter Accuracy führt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T16:39:24.042157Z",
     "start_time": "2021-12-14T16:39:23.982852Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8810020876826722"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigt.accuracy(brown_testgold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Nun bedeutet \"mehr\" nicht notwendigerweise \"besser\". Wenn wir nämlich stattdessen einen **Trigram-Tagger** implementieren und damit mehr Kontext berücksichtigen, dann erhalten wir im Vergleich zum Bigram-Tagger eine leicht *verringerte* Accuracy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T16:39:46.992972Z",
     "start_time": "2021-12-14T16:39:45.941989Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8757279419843973"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigt = NgramTagger(3, brown_train, backoff=ut)\n",
    "trigt.accuracy(brown_testgold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Der Grund dafür ist die unvermeidbare Datenknappheit: Der Trigram-Tagger wird häufiger als der Bigram-Tagger in die Situation kommen, dass ein Trigram in den Lerndaten nicht gesehen wurde und daher das Ergebnis des Backoff-Taggers übernommen werden muss. Die Accuracy der N-Gram-Tagger nähert sich also mit wachsendem N der Accuracy des Backoffs an."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <span style=\"color:red\">Aufgaben II</span>\n",
    "\n",
    "Wir sind nun bereit für den letzten Schritt.\n",
    "\n",
    "<span style=\"color:red\">A2:</span> Implementieren Sie einen POS-Tagger `ngt`, der die Tagger per `backoff` so kombiniert, dass die Accuracy besser als bei `utplus` oben ausfällt! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T16:42:09.681401Z",
     "start_time": "2021-12-14T16:41:59.685620Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lösung A2\n",
    "\n",
    "# Unigram-Tagger \n",
    "unigram_tagger = UnigramTagger(brown_train, backoff=rt)\n",
    "\n",
    "# Ngram-Tagger: Bigram und Trigram taggers mit backoff Kombinieren\n",
    "bigram_tagger = NgramTagger(2, brown_train, backoff=unigram_tagger)\n",
    "trigram_tagger = NgramTagger(3, brown_train, backoff=bigram_tagger)\n",
    "\n",
    "# Final POS Tagger (ngt)\n",
    "ngt = trigram_tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-11T09:17:14.114297Z",
     "start_time": "2021-12-11T09:17:14.035893Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |      N      V                                                C             P        |\n",
      "     |      O      E      A      D             A      A      P      O      N      R        |\n",
      "     |      U      R      D      E             D      D      R      N      U      O        |\n",
      "     |      N      B      P      T      .      J      V      T      J      M      N      X |\n",
      "-----+-------------------------------------------------------------------------------------+\n",
      "NOUN | <30.1%>  0.6%      .      .      .   0.2%      .      .      .   0.0%   0.0%      . |\n",
      "VERB |   1.0% <14.5%>  0.0%      .      .   0.0%   0.0%      .      .      .      .      . |\n",
      " ADP |      .   0.0% <12.0%>  0.0%      .      .   0.1%   0.6%      .      .   0.1%      . |\n",
      " DET |   0.0%      .      . <11.5%>     .      .   0.0%      .      .      .      .      . |\n",
      "   . |      .      .      .      . <10.8%>     .      .      .      .      .      .      . |\n",
      " ADJ |   1.3%   0.1%      .      .      .  <5.3%>  0.1%      .      .      .      .   0.0% |\n",
      " ADV |   0.0%   0.0%   0.1%   0.0%      .   0.2%  <2.6%>  0.0%      .      .      .      . |\n",
      " PRT |      .      .   0.4%      .      .   0.0%   0.0%  <1.8%>     .      .      .      . |\n",
      "CONJ |      .      .      .      .      .      .      .      .  <2.2%>     .      .      . |\n",
      " NUM |   0.1%      .      .      .      .      .      .      .      .  <1.9%>     .      . |\n",
      "PRON |      .      .   0.1%      .      .      .      .      .      .      .  <1.9%>     . |\n",
      "   X |   0.0%      .      .      .      .      .      .      .      .      .      .  <0.0%>|\n",
      "-----+-------------------------------------------------------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Accuracy: 0.9470387869464894\n"
     ]
    }
   ],
   "source": [
    "# Konfusionsmatrix\n",
    "\n",
    "testtags = [str(tag) for sent in brown_test for word,tag in ngt.tag(sent)]\n",
    "goldtags = [tag for sent in brown_testgold for word,tag in sent]\n",
    "\n",
    "cm = ConfusionMatrix(goldtags, testtags)\n",
    "\n",
    "print(cm.pretty_format(show_percents=True, values_in_chart=True, truncate=15, sort_by_count=True))\n",
    "print(\"Accuracy: {}\".format(ngt.accuracy(brown_testgold)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Achtung Domänenabhängigkeit!\n",
    "\n",
    "Bei aller Euphorie: Die Performance-Ergebnisse gelten nur für eine bestimmte **Domäne**, d.h. eine bestimmte Sprache und Textsorte. \n",
    "\n",
    "Angewandt auf eine andere Domäne als \"news\" wird die Accuracy *wahrscheinlich* dahinschmelzen. Probieren wir es aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domäne               Accuracy  \n",
      "------               --------  \n",
      "adventure            0.919442761962447\n",
      "belles_lettres       0.9181205804871285\n",
      "editorial            0.9242743977663788\n",
      "fiction              0.9202634038079663\n",
      "government           0.9282770226906456\n",
      "hobbies              0.9095148460744429\n",
      "humor                0.9172620419451486\n",
      "learned              0.914843200211119\n",
      "lore                 0.9217581301734377\n",
      "mystery              0.9229827353985551\n",
      "news                 0.9794140461841399\n",
      "religion             0.9194141983299069\n",
      "reviews              0.9120970911949685\n",
      "romance              0.9181400131387278\n",
      "science_fiction      0.9192812715964064\n"
     ]
    }
   ],
   "source": [
    "print(\"{:20} {:10}\".format(\"Domäne\",\"Accuracy\"))\n",
    "print(\"{:20} {:10}\".format(\"------\",\"--------\"))\n",
    "\n",
    "for category in brown.categories():\n",
    "    print(\"{:20} {:10}\".format(category, ngt.accuracy(\n",
    "    brown.tagged_sents(categories=category, tagset='universal'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Weitere Arten von Tagger\n",
    "\n",
    "Neben den N-Gram-Taggern gibt es noch eine Reihe weiterer stochastischer Tagger in NLTK, die wesentlich elaborierter sind: \n",
    "- [Hidden Markov Models](https://www.nltk.org/api/nltk.tag.hmm.html?highlight=markov#nltk.tag.hmm.HiddenMarkovModelTagger) ($\\to$ nächste Sitzung)\n",
    "- [Conditional Random Fields](https://www.nltk.org/api/nltk.tag.crf.html?highlight=crf#module-nltk.tag.crf) ($\\to$ nächste Sitzung)\n",
    "- [Perceptron](https://www.nltk.org/api/nltk.tag.perceptron.html?highlight=tagger#module-nltk.tag.perceptron)\n",
    "- [\"klassifikationsbasiert\"](https://www.nltk.org/api/nltk.tag.sequential.html?highlight=tagger#nltk.tag.sequential.ClassifierBasedPOSTagger)\n",
    "\n",
    "Wir werden eine Auswahl dieser Tagger in der nächsten Sitzung noch behandeln. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Brill-Tagger\n",
    "\n",
    "Zum Abschluss möchte ich auf einen besonderen Tagger eingehen, der ebenfalls statistische und regelbasierte Ansätze verbinden kann: der [**Brill-Tagger**](https://en.wikipedia.org/wiki/Brill_tagger), benannt nach Eric Brill und entstanden Anfang der 90er ([Brill 1992](https://aclanthology.org/H92-1022)). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Das besondere an diesem Tagger ist, dass er zwei Verarbeitungsschritte hat: \n",
    "1. Zunächst wird mit einem bestehenden **Baseline-Tagger** eine Eingabe verarbeitet. \n",
    "2. Dann wird die Ausgabe mithilfe von Korrekturregeln, den sogenannten **Patches**, verbessert.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Die Patches werden automatisch aus den Fehlern des Baseline-Taggers in einem Testset induziert, die man ja anhand der Golddaten ermitteln kann. Ein Patch hat die Form einer bedingten Ersetzungsregel und kann z.B. folgendermaßen ausehen:\n",
    "\n",
    "    NOUN->ADP if Word:of@[0]       ('Ersetze NOUN durch ADP, falls die Wortform /of/ ist.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Der Bedingungsteil der Patches kann prinzipiell beliebige Kontextinformationen und deren Kombination enthalten. Um den Suchraum bei der Induktion einzugrenzen, muss daher zuvor die zur Verfügung stehende Kontextinformation durch **Patch Templates** festgelegt werden. Dies erfolgt manuell, wobei der Aufwand und die Fehleranfälligkeit (es gibt keine schädlichen Patch Templates) sehr gering ist. \n",
    "\n",
    "NLTK beinhaltet z.B. die 24 Templates des ursprünglichen Brill-Tagger:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-11T11:19:34.237466Z",
     "start_time": "2021-12-11T11:19:34.230610Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Template(Pos([-1])),\n",
       " Template(Pos([1])),\n",
       " Template(Pos([-2])),\n",
       " Template(Pos([2])),\n",
       " Template(Pos([-2, -1])),\n",
       " Template(Pos([1, 2])),\n",
       " Template(Pos([-3, -2, -1])),\n",
       " Template(Pos([1, 2, 3])),\n",
       " Template(Pos([-1]),Pos([1])),\n",
       " Template(Pos([-2]),Pos([-1])),\n",
       " Template(Pos([1]),Pos([2])),\n",
       " Template(Word([-1])),\n",
       " Template(Word([1])),\n",
       " Template(Word([-2])),\n",
       " Template(Word([2])),\n",
       " Template(Word([-2, -1])),\n",
       " Template(Word([1, 2])),\n",
       " Template(Word([-1, 0])),\n",
       " Template(Word([0, 1])),\n",
       " Template(Word([0])),\n",
       " Template(Word([-1]),Pos([-1])),\n",
       " Template(Word([1]),Pos([1])),\n",
       " Template(Word([0]),Word([-1]),Pos([-1])),\n",
       " Template(Word([0]),Word([1]),Pos([1]))]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.tag.brill.brill24()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Liegen die Templates in dieser Form vor, kann der Brill-Tagger mithilfe der Ausgabe eines Baseline-Taggers die Patches induzieren. Hierfür steht in NLTK die Klasse [`BrillTaggerTrainer`](https://www.nltk.org/api/nltk.tag.brill_trainer.html?highlight=taggertrainer#nltk.tag.brill_trainer.BrillTaggerTrainer) zur Verfügung. Mittels der Funktion [`train(train_sents, max_rules=200, min_score=2, min_acc=None)`](https://www.nltk.org/api/nltk.tag.brill_trainer.html?highlight=taggertrainer#nltk.tag.brill_trainer.BrillTaggerTrainer.train) wird der Trainingsvorgang angestoßen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-11T11:32:36.075602Z",
     "start_time": "2021-12-11T11:32:05.507077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TBL train (fast) (seqs: 4223; tokens: 91453; tpls: 24; min score: 2; min acc: None)\n",
      "Finding initial useful rules...\n",
      "    Found 223614 useful rules.\n",
      "\n",
      "           B      |\n",
      "   S   F   r   O  |        Score = Fixed - Broken\n",
      "   c   i   o   t  |  R     Fixed = num tags changed incorrect -> correct\n",
      "   o   x   k   h  |  u     Broken = num tags changed correct -> incorrect\n",
      "   r   e   e   e  |  l     Other = num tags changed incorrect -> incorrect\n",
      "   e   d   n   r  |  e\n",
      "------------------+-------------------------------------------------------\n",
      "38624007 1451848  | NOUN->ADP if Pos:DET@[1]\n",
      "18001800   0   0  | NOUN->CONJ if Word:and@[0]\n",
      "16701670   0   0  | NOUN->ADP if Word:of@[0]\n",
      "10861087   1 471  | NOUN->PRT if Word:to@[0]\n",
      "10661066   0  40  | NOUN->ADP if Word:in@[0]\n",
      " 635 643   8  38  | NOUN->VERB if Word:be@[0,1]\n",
      " 618 638  20 135  | NOUN->VERB if Word:was@[-1,0]\n",
      " 543 543   0   1  | NOUN->ADP if Word:for@[0]\n",
      " 541 541   0   0  | NOUN->VERB if Word:is@[0]\n",
      " 456 696 240 159  | NOUN->VERB if Pos:PRT@[-1]\n",
      " 414 414   0 190  | NOUN->PRON if Word:he@[-1,0]\n",
      " 358 358   0   0  | NOUN->ADP if Word:at@[0]\n",
      " 357 357   0   1  | NOUN->DET if Word:his@[0]\n",
      " 337 338   1  11  | NOUN->VERB if Word:will@[-1,0]\n",
      " 319 319   0   0  | NOUN->PRON if Word:it@[0]\n",
      " 316 316   0 242  | NOUN->ADP if Word:that@[0]\n",
      " 314 319   5  34  | NOUN->VERB if Word:has@[-1,0]\n",
      " 312 312   0   0  | NOUN->ADP if Word:with@[0]\n",
      " 296 296   0  24  | NOUN->ADP if Word:on@[0]\n",
      " 292 292   0   0  | NOUN->VERB if Word:are@[0]\n",
      " 291 291   0   0  | NOUN->ADP if Word:by@[0]\n",
      " 265 266   1   5  | ADP->VERB if Pos:PRT@[-1]\n",
      " 255 268  13  40  | NOUN->VERB if Word:have@[-1,0]\n",
      " 251 251   0   0  | NOUN->VERB if Word:said@[0]\n",
      " 244 244   0   0  | NOUN->PRON if Word:who@[0]\n",
      " 240 244   4  43  | NOUN->VERB if Word:had@[-1,0]\n",
      " 230 245  15  57  | NOUN->VERB if Word:were@[-1,0]\n",
      " 227 227   0   0  | NOUN->. if Word:--@[0]\n",
      " 224 224   0  70  | NOUN->ADP if Word:as@[0]\n",
      " 223 223   0   0  | NOUN->DET if Word:this@[0]\n",
      " 208 208   0   0  | NOUN->ADP if Word:from@[0]\n",
      " 199 199   0   0  | NOUN->DET if Word:their@[0]\n",
      " 195 195   0  73  | NOUN->PRON if Word:they@[-1,0]\n",
      " 234 240   6  23  | PRON->VERB if Pos:PRON@[-1]\n",
      " 192 192   0   0  | NOUN->DET if Word:which@[0]\n",
      " 188 188   0   0  | ADP->CONJ if Word:and@[0]\n",
      " 176 176   0  34  | NOUN->PRON if Word:He@[-1,0]\n",
      " 173 173   0 108  | NOUN->PRON if Word:I@[-1,0]\n",
      " 151 154   3  83  | NOUN->ADV if Word:not@[-1,0]\n",
      " 149 157   8   0  | NOUN->NUM if Word:one@[0]\n",
      " 147 148   1  27  | NOUN->DET if Word:its@[0,1]\n",
      " 146 146   0   0  | NOUN->NUM if Word:two@[0]\n",
      " 142 142   0   0  | ADP->VERB if Word:is@[0]\n",
      " 140 142   2   1  | NOUN->ADJ if Word:last@[0]\n",
      " 138 183  45   6  | VERB->NOUN if Word:of@[1]\n",
      " 137 137   0   6  | NOUN->CONJ if Word:but@[0]\n",
      " 134 134   0   0  | NOUN->CONJ if Word:or@[0]\n",
      " 130 130   0   0  | NOUN->ADJ if Word:new@[0]\n",
      " 129 129   0   0  | NOUN->. if Word::@[0]\n",
      " 128 128   0   0  | NOUN->ADJ if Word:other@[0]\n",
      " 117 117   0  12  | NOUN->PRT if Word:all@[0]\n",
      " 110 125  15  17  | PRON->VERB if Pos:PRON@[-1]\n",
      " 110 110   0   7  | NOUN->ADJ if Word:first@[0]\n",
      " 108 108   0   7  | NOUN->PRT if Word:up@[0]\n",
      " 106 106   0  25  | NOUN->PRT if Word:out@[0]\n",
      " 100 100   0  42  | NOUN->ADJ if Word:more@[0]\n",
      "  99  99   0  13  | NOUN->PRON if Word:It@[-1,0]\n",
      "  94  94   0   0  | NOUN->ADP if Word:than@[0]\n",
      "  93 186  93 189  | NOUN->VERB if Pos:PRON@[-2,-1]\n",
      "  90  90   0   0  | NOUN->NUM if Word:three@[0]\n",
      "  88 101  13   2  | ADP->VERB if Pos:PRON@[-1] & Pos:DET@[1]\n",
      "  88  88   0  18  | NOUN->PRT if Word:there@[0]\n",
      "  87  87   0   1  | NOUN->CONJ if Word:But@[0,1]\n",
      "  83  87   4   4  | NOUN->ADJ if Word:New@[0,1]\n",
      "  82  82   0   0  | NOUN->. if Word:?@[-1,0]\n",
      "  81  81   0   0  | NOUN->ADP if Word:In@[0]\n",
      "  81 314 233   9  | VERB->NOUN if Pos:NOUN@[-1] & Pos:.@[1]\n",
      "  90  90   0   0  | NOUN->VERB if Word:said@[0]\n",
      "  77  77   0  21  | NOUN->PRON if Word:him@[0,1]\n",
      "  77  77   0   0  | NOUN->ADV if Word:when@[0]\n",
      "  77  77   0   0  | NOUN->DET if Word:any@[0]\n",
      "  77  77   0   0  | NOUN->DET if Word:some@[0]\n",
      "  75  75   0   4  | NOUN->VERB if Word:can@[-1,0]\n",
      "  75  75   0   0  | NOUN->ADP if Word:after@[0]\n",
      "  74  74   0  21  | NOUN->PRON if Word:them@[0,1]\n",
      "  74  74   0  20  | NOUN->DET if Word:her@[0]\n",
      "  72  85  13   0  | VERB->NOUN if Pos:DET@[-1] & Pos:ADP@[1]\n",
      "  72  73   1  11  | NOUN->ADV if Word:also@[-1,0]\n",
      "  72  72   0  27  | NOUN->PRON if Word:we@[-1,0]\n",
      "  71  79   8   5  | VERB->NOUN if Pos:DET@[-1] & Pos:.@[1]\n",
      "  65  65   0   0  | ADP->VERB if Word:will@[-1]\n",
      "  65  65   0   2  | NOUN->DET if Word:no@[0]\n",
      "  65  65   0   0  | VERB->ADV if Word:not@[0]\n",
      "  64  69   5   2  | NOUN->ADJ if Word:American@[0,1]\n",
      "  63  63   0   0  | NOUN->DET if Word:what@[0]\n",
      "  62  62   0   0  | ADP->VERB if Word:was@[0]\n",
      "  60  60   0   0  | NOUN->ADV if Word:here@[0]\n",
      "  59  59   0   0  | NOUN->DET if Word:This@[0,1]\n",
      "  58  58   0  19  | NOUN->PRON if Word:They@[-1,0]\n",
      "  58  58   0   0  | NOUN->NUM if Word:four@[0]\n",
      "  56  56   0   0  | NOUN->ADP if Word:if@[0]\n",
      "  55  55   0   0  | NOUN->VERB if Word:made@[0]\n",
      "  54  54   0  34  | NOUN->ADV if Word:so@[-1,0]\n",
      "  53  53   0   0  | VERB->ADP if Word:during@[-1,0]\n",
      "  53  53   0   0  | NOUN->ADP if Word:into@[0]\n",
      "  51  51   0   0  | NOUN->ADP if Word:per@[0]\n",
      "  50  50   0   0  | NOUN->ADJ if Word:many@[0]\n",
      "  50  50   0   0  | NOUN->DET if Word:each@[0]\n",
      "  49  56   7  14  | NOUN->VERB if Word:would@[-2,-1]\n",
      "  49  50   1   9  | NOUN->ADV if Word:now@[-1,0]\n",
      "  49  49   0   0  | ADP->. if Word:--@[0]\n",
      "  48  48   0  19  | NOUN->ADV if Word:then@[-1,0]\n",
      "  48  48   0   8  | NOUN->ADP if Word:before@[0]\n",
      "  47  47   0  28  | NOUN->PRON if Word:you@[-1,0]\n",
      "  47  47   0   0  | NOUN->VERB if Word:may@[-1,0]\n",
      "  47  47   0   0  | NOUN->DET if Word:our@[0]\n",
      "  47  47   0   0  | NOUN->DET if Word:these@[0]\n",
      "  47  47   0   0  | NOUN->DET if Word:those@[0]\n",
      "  46  46   0   6  | NOUN->PRT if Word:There@[-1,0]\n",
      "  45  45   0   0  | NOUN->ADV if Word:where@[0]\n",
      "  43  43   0   2  | ADP->VERB if Word:has@[-1,0]\n",
      "  42  42   0  13  | NOUN->PRON if Word:she@[-1,0]\n",
      "  42  42   0  37  | NOUN->ADP if Word:about@[0]\n",
      "  42  42   0   0  | VERB->NOUN if Word:sales@[0]\n",
      "  40  47   7  41  | NOUN->ADJ if Word:most@[-1,0]\n",
      "  39  39   0   0  | ADP->ADV if Word:when@[0]\n",
      "  39  39   0   0  | NOUN->ADJ if Word:National@[0]\n",
      "  39  39   0   0  | NOUN->ADP if Word:against@[0]\n",
      "  39  39   0   0  | NOUN->DET if Word:both@[0]\n",
      "  38  87  49  10  | PRON->VERB if Pos:PRON@[-1]\n",
      "  38  44   6   0  | VERB->NOUN if Pos:NUM@[-1] & Pos:.@[1]\n",
      "  38  38   0   0  | ADV->NOUN if Word:family@[-1,0]\n",
      "  38  44   6   3  | NOUN->ADJ if Word:Democratic@[0,1]\n",
      "  37  41   4   6  | NOUN->ADJ if Word:good@[0,1]\n",
      "  37  37   0   0  | NOUN->. if Word:'@[0]\n",
      "  37  40   3   4  | NOUN->ADJ if Word:high@[0]\n",
      "  37  37   0   0  | NOUN->ADJ if Word:own@[0]\n",
      "  37  37   0   0  | NOUN->CONJ if Word:And@[0]\n",
      "  37  71  34  14  | VERB->NOUN if Pos:CONJ@[1] & Pos:NOUN@[2]\n",
      "  37  37   0   9  | NOUN->PRT if Word:off@[0]\n",
      "  36  36   0   0  | NOUN->. if Word:!@[-1,0]\n",
      "  36  51  15  43  | NOUN->VERB if Word:be@[-1,0]\n",
      "  36  37   1  12  | NOUN->VERB if Word:been@[-1,0]\n",
      "  36  37   1   3  | NOUN->ADJ if Word:young@[0,1]\n",
      "  36  36   0  13  | NOUN->ADV if Word:p.m.@[0,1]\n",
      "  35  35   0   3  | NOUN->PRON if Word:She@[-1,0]\n",
      "  35  38   3   1  | NOUN->VERB if Word:did@[-1,0]\n",
      "  35  56  21  27  | ADV->VERB if Pos:VERB@[-2] & Pos:ADV@[-1]\n",
      "  35  35   0   1  | NOUN->ADJ if Word:same@[0,1]\n",
      "  35  35   0   1  | NOUN->ADJ if Word:such@[0]\n",
      "  35  35   0   4  | NOUN->ADV if Word:even@[0]\n",
      "  35  35   0   0  | NOUN->NUM if Word:five@[0]\n",
      "  34  36   2   1  | ADP->VERB if Word:had@[-1,0]\n",
      "  34  34   0   8  | NOUN->ADV if Word:well@[-1,0]\n",
      "  34  34   0   0  | VERB->NOUN if Word:James@[-1,0]\n",
      "  34  34   0   2  | NOUN->ADJ if Word:next@[0]\n",
      "  34  34   0   0  | NOUN->ADJ if Word:special@[0]\n",
      "  34  34   0   1  | NOUN->ADP if Word:since@[0]\n",
      "  34  34   0   0  | NOUN->ADP if Word:without@[0]\n",
      "  33  33   0   0  | NOUN->ADV if Word:however@[-1,0]\n",
      "  33  33   0  15  | NOUN->ADV if Word:too@[-1,0]\n",
      "  33  33   0  20  | NOUN->ADP if Word:over@[0]\n",
      "  33  33   0   0  | NOUN->ADV if Word:ago@[0]\n",
      "  33  44  11   0  | NOUN->ADV if Word:back@[0]\n",
      "  32  32   0   1  | ADP->VERB if Word:said@[0,1]\n",
      "  32  36   4   1  | NOUN->ADJ if Word:annual@[0,1]\n",
      "  32  73  41   7  | VERB->NOUN if Pos:ADJ@[-1]\n",
      "  32  32   0   1  | NOUN->ADJ if Word:few@[0,1]\n",
      "  32  32   0   0  | NOUN->ADJ if Word:big@[0]\n",
      "  32  32   0   1  | NOUN->ADJ if Word:third@[0]\n",
      "  31  31   0   0  | NOUN->VERB if Word:might@[-1,0]\n",
      "  31  31   0   0  | NOUN->ADV if Word:When@[0,1]\n",
      "  31  32   1   1  | NOUN->ADJ if Word:second@[0]\n",
      "  31  31   0   1  | NOUN->ADP if Word:under@[0]\n",
      "  31  31   0   0  | NOUN->CONJ if Word:&@[0]\n",
      "  31  31   0   1  | NOUN->PRT if Word:down@[0]\n",
      "  30  30   0   7  | NOUN->ADV if Word:just@[-1,0]\n",
      "  30  31   1   0  | NOUN->VERB if Word:must@[-1,0]\n",
      "  30  31   1   1  | VERB->PRON if Word:him@[-1,0]\n",
      "  29  49  20   0  | VERB->NOUN if Pos:ADP@[-1] & Pos:.@[1]\n",
      "  29  29   0   3  | NOUN->VERB if Word:came@[-1,0]\n",
      "  29  32   3   0  | NOUN->ADJ if Word:national@[0,1]\n",
      "  29  29   0   2  | NOUN->DET if Word:another@[0,1]\n",
      "  29  29   0   0  | NOUN->PRON if Word:We@[0,1]\n",
      "  29  29   0   0  | NOUN->ADJ if Word:several@[0]\n",
      "  29  29   0   8  | NOUN->ADP if Word:because@[0]\n",
      "  28  28   0   0  | NOUN->ADV if Word:how@[0]\n",
      "  28  28   0   0  | NOUN->NUM if Word:million@[0]\n",
      "  27  44  17  16  | ADV->ADJ if Word:the@[-1]\n",
      "  27  27   0   4  | NOUN->ADV if Word:later@[-1,0]\n",
      "  27  29   2   2  | NOUN->ADJ if Word:foreign@[0,1]\n",
      "  27  28   1   2  | NOUN->ADJ if Word:higher@[0,1]\n",
      "  27  27   0   0  | VERB->PRON if Word:them@[0]\n",
      "  26  26   0   0  | ADP->VERB if Word:would@[-1]\n",
      "  26  32   6   3  | VERB->NOUN if Word:are@[1]\n",
      "  26  26   0   1  | NOUN->ADJ if Word:great@[0,1]\n",
      "  26  27   1   0  | NOUN->ADJ if Word:North@[0]\n",
      "  26  26   0   0  | NOUN->ADJ if Word:small@[0]\n",
      "  26  26   0   0  | NOUN->ADV if Word:again@[0]\n",
      "  26  26   0   0  | NOUN->VERB if Word:began@[0]\n",
      "  25  48  23   0  | ADP->ADV if Word:as@[2]\n",
      "  25  25   0   3  | NOUN->VERB if Word:got@[-1,0]\n",
      "  25  25   0   9  | VERB->DET if Word:no@[-1,0]\n",
      "  25  26   1   0  | NOUN->ADJ if Word:political@[0,1]\n",
      "  25  25   0   0  | NOUN->DET if Word:That@[0,1]\n",
      "  25  25   0   2  | NOUN->DET if Word:my@[0,1]\n",
      "  25  25   0   0  | NOUN->NUM if Word:eight@[0]\n",
      "  24  24   0   0  | NOUN->ADJ if Word:former@[0,1]\n",
      "  24  24   0   0  | NOUN->DET if Word:His@[0,1]\n",
      "  24  29   5   0  | NOUN->ADJ if Word:local@[0]\n"
     ]
    }
   ],
   "source": [
    "baseline = rt\n",
    "learndata = brown_train\n",
    "templates = nltk.tag.brill.brill24()\n",
    "\n",
    "btt = nltk.tag.brill_trainer.BrillTaggerTrainer(baseline, templates, trace=3)\n",
    "brilltagger = btt.train(learndata, max_rules=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-11T11:35:30.280962Z",
     "start_time": "2021-12-11T11:35:30.083954Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy des Baseline-Taggers:  0.566311394352269\n",
      "Accuracy des Brill-Taggers:  0.8668278211185584\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy des Baseline-Taggers: \",baseline.accuracy(brown_testgold))\n",
    "print(\"Accuracy des Brill-Taggers: \",brilltagger.accuracy(brown_testgold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Der Brill-Tagger hat im Allgemeinen drei Vorteile (siehe [Brill 1992](https://aclanthology.org/H92-1022)):\n",
    "- Portabilität: Baseline-Tagger können leicht für andere Tagsets, Textgenres und Sprachen angepasst werden.\n",
    "- Geringer Speicherverbrauch: Die Anzahl der Regeln ist sehr gering verglichen mit N-Gram-Taggern. \n",
    "- Transparenz: Die Patches sind leicht verständlich und erweiterbar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Appendix: Konkordanzen\n",
    "\n",
    "Konkordanzen sind alignierte Trefferlisten ([Wikipedia](https://de.wikipedia.org/wiki/Konkordanz_\\(Textwissenschaft\\)#Konkordanzsoftware)), wobei die Suchanfragen nicht nur Wortformen, sondern auch POS-Tags enthalten können. Konkordanzen sind in der Linguistik sehr wichtig für die Datenrecherche, denn sie stellen gefundene Belege übersichtlich dar. \n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/de/d/d4/Konkordanz_Nationalrat.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "NLTK enthält ebenfalls einen sogenannten **Concordancer**, wobei hierfür ein Corpus zunächst in ein [Text-Objekt](https://www.nltk.org/api/nltk.text.html?highlight=text#nltk.text.Text) überführt werden muss. Mittels der Funktion [`concordance(word, width=79, lines=25)`](https://www.nltk.org/api/nltk.text.html?highlight=text#nltk.text.Text.concordance) können dann Konkordanzen ausgelesen werden.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T08:22:41.374288Z",
     "start_time": "2021-12-10T08:22:35.879044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 13 of 13 matches:\n",
      " couple of weeks ago , he scored a monstrous 12 on a par-5 hole . It made him h\n",
      "ework or his plates feel loose and monstrous . His bifocals blur . His legs sud\n",
      "he Soviet Union stands guilty of a monstrous crime against the human race . But\n",
      "sment of the joke gone wrong , the monstrous image of the fat man dressed up as\n",
      "ement was for Copernicus literally monstrous : `` With ( the Ptolemaists ) it i\n",
      "ceptions were quick and his energy monstrous , but these qualities were sapped \n",
      "U.S. . It was `` the creation of a monstrous historical period wherein it thoug\n",
      "me might be 38 , and occasional `` monstrous freaks '' over 50 . He rejects dim\n",
      "t least 37 feet and the 50-foot `` monstrous freaks '' intimated by Heuvelmans \n",
      "st unnatural actions , of the most monstrous murders , told with the most spont\n",
      "spicion darted into his mind . Too monstrous , of course . Mae wouldn't have pl\n",
      " . The sound was coming nearer . A monstrous shadow fell across the illuminated\n",
      "And once begun , had grown to such monstrous proportions . The pair of white co\n"
     ]
    }
   ],
   "source": [
    "from nltk.text import Text\n",
    "text = Text(brown.words())\n",
    "text.concordance(\"monstrous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Darüber hinaus steht eine GUI zur Verfügung, bei dem auch die Wortarten angezeigt und in die Suche einbezogen werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T08:41:41.433725Z",
     "start_time": "2021-12-10T08:22:41.376185Z"
    }
   },
   "outputs": [],
   "source": [
    "#nltk.download('universal_tagset')\n",
    "#nltk.app.concordance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Literaturangaben\n",
    "\n",
    "- Brill, Eric. 1992. A simple rule-based part of speech tagger. In Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, February 23–26, 1992, 112–116. Association for Computational Linguistics. https://aclanthology.org/H92-1022."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
